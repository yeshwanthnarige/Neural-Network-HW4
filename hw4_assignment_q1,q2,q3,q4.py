# -*- coding: utf-8 -*-
"""HW4 assignment - Q1,Q2,Q3,Q4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gGBqwKA40ttH3Ue67SAZnvcPX1cJiFM5

**Q1 NLP Preprocessing Pipeline**
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Download required NLTK data (only the first time)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Download the punkt_tab data package

def nlp_preprocessing_pipeline(sentence):
    # Step 1: Tokenize
    tokens = word_tokenize(sentence)
    print("Original Tokens:", tokens)

    # Step 2: Remove Stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
    print("Tokens Without Stopwords:", filtered_tokens)

    # Step 3: Apply Stemming
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
    print("Stemmed Words:", stemmed_tokens)

# Example usage
sentence = "NLP techniques are used in virtual assistants like Alexa and Siri."
nlp_preprocessing_pipeline(sentence)

"""Short Answer Questions

**1** . Stemming vs. Lemmatization
Stemming chops off word endings to reach a root form (not always a valid word).

e.g., running → run (PorterStemmer might output run or runn)

Lemmatization uses vocabulary and grammar to return a base or dictionary form (lemma).

e.g., running → run (linguistically correct base form)

Example with "running":

Stemming: running → run or runn

Lemmatization: running → run

**2. **Stopword Removal: Helpful vs. Harmful
Helpful when: Focusing on core semantics (e.g., search engines, topic modeling).

e.g., In "The dog barked at the stranger", removing "the" helps isolate key content.

Harmful when: Stopwords carry important meaning (e.g., sentiment analysis, translation, question answering).

e.g., In "I am not happy", removing "not" changes sentiment entirely.

**Q2: Named Entity Recognition (NER) with spaCy**
"""

import spacy

# Load English model
nlp = spacy.load("en_core_web_sm")

# Input sentence
sentence = "Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009."

# Process sentence
doc = nlp(sentence)

# Print Named Entities
for ent in doc.ents:
    print(f"Entity Text: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}")

"""Short Answer Questions.

**1.** How does NER differ from POS tagging in NLP?
Named Entity Recognition (NER) identifies real-world entities like names of people, places, organizations, dates, etc.

e.g., "Barack Obama" → PERSON, "2009" → DATE

Part-of-Speech (POS) Tagging assigns grammatical roles to words like noun, verb, adjective, etc.

e.g., "Obama" → NNP (proper noun), "served" → VBD (past tense verb)

-> NER focuses on semantic meaning, whereas POS tagging focuses on syntactic structure.

**2.** Two Real-World Applications of NER
a. Financial News & Market Analysis

NER extracts company names, stock tickers, acquisition dates, CEO names, etc., to support algorithmic trading or news summarization.

Example: “Apple acquired Beats in 2014” → Apple (ORG), Beats (ORG), 2014 (DATE)

b. Search Engines and Question Answering

Helps improve relevance by identifying entities in queries and documents.

Example: "Who won the Nobel Prize in 2009?" → Recognizes Nobel Prize (WORK_OF_ART), 2009 (DATE)

**Q3: Scaled Dot-Product Attention**
"""

import numpy as np

def scaled_dot_product_attention(Q, K, V):
    """
    Scaled Dot-Product Attention

    Args:
    Q (numpy.ndarray): Query matrix of shape (batch_size, seq_len, d)
    K (numpy.ndarray): Key matrix of shape (batch_size, seq_len, d)
    V (numpy.ndarray): Value matrix of shape (batch_size, seq_len, d)

    Returns:
    tuple: attention_weights (numpy.ndarray), output (numpy.ndarray)
    """
    # Step 1: Compute dot product of Q and Kᵀ
    attn_scores = np.dot(Q, K.T)

    # Step 2: Scale the result by √d (where d is the key dimension)
    d = K.shape[-1]  # The dimension of the key
    scaled_scores = attn_scores / np.sqrt(d)

    # Step 3: Apply softmax to get attention weights
    attn_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)

    # Step 4: Multiply the attention weights by V to get the output
    output = np.dot(attn_weights, V)

    return attn_weights, output

# Test inputs
Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

# Get attention weights and output
attn_weights, output = scaled_dot_product_attention(Q, K, V)

# Display results
print("Attention Weights:")
print(attn_weights)
print("\nOutput:")
print(output)

"""Short Answer Questions
1. Why do we divide the attention score by √d in the scaled dot-product attention formula?
Dividing by √d (where d is the dimension of the key vector) ensures that the attention scores are not excessively large. As the dimension of the key vector increases, the dot product tends to grow, which could lead to large gradients during backpropagation. Scaling by √d helps in stabilizing the learning process and prevents numerical instability.

2. How does self-attention help the model understand relationships between words in a sentence?
Self-attention allows each word in a sentence to attend to every other word, thus capturing contextual relationships. For example, in the sentence "The cat sat on the mat," the word "cat" can attend to "sat" and "mat", helping the model understand how these words relate to each other. This is key for understanding complex sentence structures and word dependencies.

**Q4 :Sentiment Analysis using HuggingFace Transformers**
"""

from transformers import pipeline

# Load pre-trained sentiment analysis pipeline
sentiment_analyzer = pipeline("sentiment-analysis")

# Input sentence
sentence = "Despite the high price, the performance of the new MacBook is outstanding."

# Perform sentiment analysis
result = sentiment_analyzer(sentence)

# Display the result
label = result[0]['label']
confidence_score = result[0]['score']

print(f"Sentiment: {label}")
print(f"Confidence Score: {confidence_score:.4f}")

"""Short Answer Questions
1. What is the main architectural difference between BERT and GPT? Which uses an encoder and which uses a decoder?
BERT (Bidirectional Encoder Representations from Transformers) is an encoder-only model. It is designed to capture context from both the left and right of a word (bidirectional context). BERT is primarily used for tasks like classification, question answering, and named entity recognition (NER).

 GPT (Generative Pre-trained Transformer) is a decoder-only model. It generates text in a left-to-right manner (unidirectional). GPT is primarily used for language generation tasks such as text completion and conversation generation.

2. Explain why using pre-trained models (like BERT or GPT) is beneficial for NLP applications instead of training from scratch.
Using pre-trained models offers several benefits:

 Time and Resources: Training an NLP model from scratch requires enormous amounts of data and computational resources. Pre-trained models have already learned from massive datasets, which significantly reduces the time and resources needed for training.

 Generalization: Pre-trained models like BERT and GPT have been trained on diverse text data, allowing them to generalize better on various NLP tasks without needing task-specific training data.

 Transfer Learning: Pre-trained models provide a solid foundation that can be fine-tuned on specific tasks with relatively small datasets, making them highly adaptable and efficient for a wide range of NLP tasks.
"""